
import time
from selenium import webdriver
from selenium.webdriver.chrome.webdriver import WebDriver
from selenium.webdriver.common.by import By
from selenium.common.exceptions import NoSuchElementException

# Create a Chrome WebDriver instance
driver:WebDriver = webdriver.Chrome("/Users/shankar/Desktop/ChromeDriver/chromedriver")

# Navigate to the Google search results page for data scientist jobs in Newport Beach, California
driver.get("https://in.indeed.com/jobs?q=product+manager&l=Bangalore%2C+Karnataka&vjk=82c98bcd45739295&advn=9740249730601869")

# /html/body/main/div/div[1]/div/div/div[5]/div[1]/div[5]/div/ul/li[1]
# //*[@id="mosaic-provider-jobcards"]/ul/li[1]

# Define XPath expressions for different pieces of information
xpaths = {
    'Role': ".//td[@class='resultContent']/div[1]/h2/a/span",
    'Company': ".//td[@class='resultContent']/div[2]/span[1]",
    'Location': ".//td[@class='resultContent']/div[2]/div",
    'Posted': ".//span[@class='date']",
    'Full / Part Time': ".//td[@class='resultContent']/div[3]/div[2]/div",
    'Salary': ".//td[@class='resultContent']/div[3]/div[1]/div"
}
# //*[@id="mosaic-provider-jobcards"]/ul/li[1]/div/div[1]/div/div[1]/div/table[1]/tbody/tr/td/div[3]/div[1]/div/text()
# //*[@id="mosaic-provider-jobcards"]/ul/li[1]/div/div[1]/div/div[1]/div/table[1]/tbody/tr/td/div[2]/div
# Create an empty dictionary to store extracted data
data = {key: [] for key in xpaths}

# Initialize variables for tracking jobs
jobs_done = 0

# Scroll down repeatedly to load all job listings
while True:
    # Find all job listing elements using the defined XPath
    lis = driver.find_elements(By.XPATH, "//*[@id='mosaic-provider-jobcards']/ul/li")

    # Break the loop if all job listings have been processed
    if jobs_done >= len(lis):
        break

    # Iterate through each job listing element
    for li in lis[jobs_done:]:
        # Scroll the page to bring the current element into view
        # driver.execute_script('arguments[0].scrollIntoView({block: "center", behavior: "smooth"});', li)

        # Initialize a dictionary to store extracted data for the current listing


        # Iterate through the defined XPath keys to extract information in the specified order
        for key in ['Role', 'Company', 'Location', 'Posted', 'Full / Part Time', 'Salary']:
            try:
                # Extract data from the current job listing using the XPath
                element = li.find_element(By.XPATH, xpaths[key])
                t = element.text
            except NoSuchElementException:
                # Handle missing data by assigning a placeholder
                t = '*missing data*'

            # Store the extracted data in the dictionary
            data[key].append(t)

        # Append the job data to the main data dictionary
        data['Role'].append(data['Role'])
        data['Company'].append(data['Company'])
        data['Location'].append(data['Location'])
        data['Posted'].append(data['Posted'])
        data['Full / Part Time'].append(data['Full / Part Time'])
        data['Salary'].append(data['Salary'])

        # Increment the counter for processed jobs and print progress
        jobs_done += 1
        # print(f'{jobs_done=}', end='\r')

        # Introduce a short delay before processing the next listing
        time.sleep(.2)

# Print the extracted information for each job listing
for i in range(len(data['Role'])):
    print(f"Role: {data['Role'][i]}")
    print(f"Company: {data['Company'][i]}")
    print(f"Location: {data['Location'][i]}")
    print(f"Posted: {data['Posted'][i]}")
    print(f"Full / Part Time: {data['Full / Part Time'][i]}")
    print(f"Salary: {data['Salary'][i]}")
    print("=" * 50)

# Close the WebDriver
driver.quit()
